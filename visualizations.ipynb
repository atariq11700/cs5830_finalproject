{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from models.CNN_model import CNN\n",
    "from models.CNN_SE_model import CNN_SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_BASE = \"interpretability-images\"\n",
    "\n",
    "MODEL_CNN_PATH = \"saved-models/cnn.pt\"\n",
    "MODEL_CNN_SE_PATH = \"saved-models/cnn-se.pt\"\n",
    "\n",
    "cnn: CNN = torch.load(MODEL_CNN_PATH).cpu()\n",
    "cnn_se: CNN_SE = torch.load(MODEL_CNN_SE_PATH).cpu()\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_convolutions(module: torch.nn.Module) -> list[torch.nn.Conv2d]:\n",
    "    convolution_layers = []\n",
    "    for module_layer in module.children():\n",
    "        if type(module_layer) == torch.nn.Conv2d:\n",
    "            convolution_layers.append(module_layer)\n",
    "\n",
    "    return convolution_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_kernels(module: torch.nn.Module, layer: int = 0, ch: int = 0, view_all_channels: bool = False):\n",
    "    output_dir = f\"{module._get_name()}-layer{layer}\"\n",
    "    if not OUTPUT_BASE in os.listdir():\n",
    "        os.mkdir(OUTPUT_BASE)\n",
    "\n",
    "    if not output_dir in os.listdir(OUTPUT_BASE):\n",
    "        os.mkdir(f\"{OUTPUT_BASE}/{output_dir}\")\n",
    "\n",
    "    convolution_layers = get_convolutions(module)\n",
    "    kernels = convolution_layers[layer].weight.detach().clone()\n",
    "\n",
    "    out_channels, in_channels, *ksize = kernels.shape\n",
    "    print(f\"Out Channels: {out_channels}, In Channels: {in_channels}, Kernel Size: {ksize}\")\n",
    "\n",
    "    if view_all_channels:\n",
    "        kernels = kernels.view(out_channels * in_channels, -1, ksize[0], ksize[1])\n",
    "    else:\n",
    "        #[out_channels, in_channels, kernel_width, kernel_height]\n",
    "        kernels = kernels[:, ch, :, :].unsqueeze(dim=1)\n",
    "\n",
    "    imgs = torchvision.utils.make_grid(kernels, padding=1, nrow=12, normalize=True)\n",
    "    imgs = imgs.permute(1,2,0)\n",
    "\n",
    "    fig_height, fig_width, fig_channels = imgs.shape\n",
    "    print(f\"Figure Height: {fig_height}, Figure Width: {fig_width}, Figure Channels: {fig_channels}\")\n",
    "\n",
    "    plt.figure(figsize=(12,(out_channels * in_channels) / 12))\n",
    "    plt.axis(\"off\")\n",
    "    image = plt.imshow(imgs)\n",
    "    plt.title(f\"{module._get_name()}, Channel: {ch if not view_all_channels else 'All'}, Layer: {layer}\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.imsave(f\"{OUTPUT_BASE}/{output_dir}/kernels-channel-{ch}.png\", image.get_array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_feature_map(module: torch.nn.Module, image_path: str, layer: int = 0):\n",
    "    output_dir = f\"{module._get_name()}-layer{layer}\"\n",
    "    if not OUTPUT_BASE in os.listdir():\n",
    "        os.mkdir(OUTPUT_BASE)\n",
    "\n",
    "    if not output_dir in os.listdir(OUTPUT_BASE):\n",
    "        os.mkdir(f\"{OUTPUT_BASE}/{output_dir}\")\n",
    "\n",
    "    convolution_layers = get_convolutions(module)\n",
    "    print(f\"Number of convolution layers in {module._get_name()}: {len(convolution_layers)}\")\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "    assert image.size == (224, 224)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.imshow(image)\n",
    "    ax1.set_title(\"Original Image\")\n",
    "    ax1.axis(\"off\")\n",
    "\n",
    "    image = data_transforms(image)\n",
    "\n",
    "    ax2.imshow(image.permute(1,2,0))\n",
    "    ax2.set_title(\"Image After Transforms\")\n",
    "    ax2.axis(\"off\")\n",
    "    plt.show()\n",
    "    fig.savefig(f\"{OUTPUT_BASE}/{output_dir}/{image_path.split('/')[-1]}-layer-{layer}-transforms.png\")\n",
    "\n",
    "    feature_maps = []\n",
    "    for conv_layer in convolution_layers:\n",
    "        image = conv_layer(image)\n",
    "        feature_maps.append(image.detach())\n",
    "\n",
    "    imgs = torchvision.utils.make_grid(feature_maps[layer].unsqueeze(1), padding=1, normalize=True)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Feature Maps for Convolution Layer:\\n{convolution_layers[layer]}\")\n",
    "    plt.imshow(imgs.permute(1,2,0))\n",
    "    plt.show()\n",
    "    fig.savefig(f\"{OUTPUT_BASE}/{output_dir}/{image_path.split('/')[-1]}-layer-{layer}-featuremaps.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature_map(cnn, \"preprocessed-data/images/Cardinalis Sinuatus/11405.jpg\", layer=0)\n",
    "view_feature_map(cnn, \"preprocessed-data/images/Cardinalis Sinuatus/11405.jpg\", layer=1)\n",
    "view_feature_map(cnn, \"preprocessed-data/images/Cardinalis Sinuatus/11405.jpg\", layer=2)\n",
    "\n",
    "view_feature_map(cnn, \"preprocessed-data/images/A380/00840.jpg\", layer=0)\n",
    "view_feature_map(cnn, \"preprocessed-data/images/A380/00840.jpg\", layer=1)\n",
    "view_feature_map(cnn, \"preprocessed-data/images/A380/00840.jpg\", layer=2)\n",
    "\n",
    "view_feature_map(cnn_se, \"preprocessed-data/images/Cardinalis Sinuatus/11405.jpg\", layer=0)\n",
    "view_feature_map(cnn_se, \"preprocessed-data/images/Cardinalis Sinuatus/11405.jpg\", layer=1)\n",
    "view_feature_map(cnn_se, \"preprocessed-data/images/Cardinalis Sinuatus/11405.jpg\", layer=2)\n",
    "\n",
    "view_feature_map(cnn_se, \"preprocessed-data/images/A380/00840.jpg\", layer=0)\n",
    "view_feature_map(cnn_se, \"preprocessed-data/images/A380/00840.jpg\", layer=1)\n",
    "view_feature_map(cnn_se, \"preprocessed-data/images/A380/00840.jpg\", layer=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_kernels(cnn, layer=0)\n",
    "view_kernels(cnn, layer=1)\n",
    "view_kernels(cnn, layer=2)\n",
    "\n",
    "view_kernels(cnn_se, layer=0)\n",
    "view_kernels(cnn_se, layer=1)\n",
    "view_kernels(cnn_se, layer=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
